# -*- coding: utf-8 -*-
"""baseline_code_kfold.py

K-Fold Cross Validation version of the document type classification baseline code.

## Improvements Applied:
- Stratified 5-Fold Cross Validation
- Better model architecture (EfficientNet)
- Larger image size (224x224)
- Enhanced data augmentation
- Learning rate scheduler
- Label smoothing loss
- Mixed precision training
- Test time augmentation (TTA)
- Model ensemble from all folds

## Contents
- Prepare Environments
- Import Library & Define Functions
- Hyper-parameters
- K-Fold Cross Validation Training
- Ensemble Inference & Save File
"""

import os
import time
import random

import timm
import torch
import albumentations as A
import pandas as pd
import numpy as np
import torch.nn as nn
from albumentations.pytorch import ToTensorV2
from torch.optim import Adam
from torch.optim.lr_scheduler import CosineAnnealingLR
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
from PIL import Image
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import StratifiedKFold
import gc

# ë¡œê·¸ ìœ í‹¸ë¦¬í‹° import
import utils.log_util as log

# ì‹œë“œë¥¼ ê³ ì •í•©ë‹ˆë‹¤.
def set_seed(seed):
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    # CUDA 10.2+ í™˜ê²½ì—ì„œ ê²°ì •ì  ì—°ì‚°ì„ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # ì™„ì „í•œ ì¬í˜„ì„±ì„ ìœ„í•œ ì„¤ì •
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # CUDA í™˜ê²½ì—ì„œ ê²°ì •ì  ì—°ì‚° ì„¤ì • (ì„ íƒì )
    try:
        torch.use_deterministic_algorithms(True)
        log.info("ì™„ì „í•œ ì¬í˜„ì„± ì„¤ì •ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        log.info(f"torch.use_deterministic_algorithms(True) ì„¤ì • ì‹¤íŒ¨: {e}")
        log.info("ê¸°ë³¸ ì¬í˜„ì„± ì„¤ì •ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")

SEED = 42
set_seed(SEED)
log.info(f"ğŸŒ± Random seed set to {SEED}")

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ë¥¼ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
os.chdir(os.path.dirname(os.path.abspath(__file__)))

# Label Smoothing Loss í´ë˜ìŠ¤ ì •ì˜
class LabelSmoothingLoss(nn.Module):
    def __init__(self, classes, smoothing=0.1):
        super(LabelSmoothingLoss, self).__init__()
        self.smoothing = smoothing
        self.classes = classes
        
    def forward(self, pred, target):
        pred = pred.log_softmax(dim=-1)
        with torch.no_grad():
            true_dist = torch.zeros_like(pred)
            true_dist.fill_(self.smoothing / (self.classes - 1))
            true_dist.scatter_(1, target.data.unsqueeze(1), 1.0 - self.smoothing)
        return torch.mean(torch.sum(-true_dist * pred, dim=-1))

# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
class ImageDataset(Dataset):
    def __init__(self, csv_data, path, transform=None):
        if isinstance(csv_data, str):
            self.df = pd.read_csv(csv_data).values
        else:
            self.df = csv_data.values
        self.path = path
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        name, target = self.df[idx]
        img = np.array(Image.open(os.path.join(self.path, name)))
        if self.transform:
            img = self.transform(image=img)['image']
        return img, target

# one epoch í•™ìŠµì„ ìœ„í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤.
def train_one_epoch(loader, model, optimizer, loss_fn, device, scaler):
    model.train()
    train_loss = 0
    preds_list = []
    targets_list = []

    pbar = tqdm(loader, desc="Training")
    for image, targets in pbar:
        image = image.to(device)
        targets = targets.to(device)

        optimizer.zero_grad()

        with autocast():
            preds = model(image)
            loss = loss_fn(preds, targets)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        train_loss += loss.item()
        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())
        targets_list.extend(targets.detach().cpu().numpy())

        pbar.set_description(f"Training - Loss: {loss.item():.4f}")

    train_loss /= len(loader)
    train_acc = accuracy_score(targets_list, preds_list)
    train_f1 = f1_score(targets_list, preds_list, average='macro')

    ret = {
        "train_loss": train_loss,
        "train_acc": train_acc,
        "train_f1": train_f1,
    }

    return ret

# ê²€ì¦ì„ ìœ„í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤.
def validate_one_epoch(loader, model, loss_fn, device):
    model.eval()
    val_loss = 0
    preds_list = []
    targets_list = []

    pbar = tqdm(loader, desc="Validation")
    with torch.no_grad():
        for image, targets in pbar:
            image = image.to(device)
            targets = targets.to(device)

            with autocast():
                preds = model(image)
                loss = loss_fn(preds, targets)

            val_loss += loss.item()
            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())
            targets_list.extend(targets.detach().cpu().numpy())

            pbar.set_description(f"Validation - Loss: {loss.item():.4f}")

    val_loss /= len(loader)
    val_acc = accuracy_score(targets_list, preds_list)
    val_f1 = f1_score(targets_list, preds_list, average='macro')

    ret = {
        "val_loss": val_loss,
        "val_acc": val_acc,
        "val_f1": val_f1,
    }

    return ret

# Test Time Augmentationì„ ìœ„í•œ ì˜ˆì¸¡ í•¨ìˆ˜
def predict_with_tta(model, loader, device, tta_count=5):
    model.eval()
    predictions = []
    
    pbar = tqdm(loader, desc="TTA Prediction")
    for image, _ in pbar:
        batch_preds = []
        for _ in range(tta_count):
            with torch.no_grad():
                with autocast():
                    pred = model(image.to(device))
                    batch_preds.append(pred.softmax(dim=1))
        
        # í‰ê· ë‚´ê¸°
        final_pred = torch.stack(batch_preds).mean(0)
        predictions.append(final_pred.cpu().numpy())
    
    return np.vstack(predictions)

# ì•™ìƒë¸” ì˜ˆì¸¡ í•¨ìˆ˜
def predict_ensemble(models, loader, device, tta_count=5):
    all_predictions = []
    
    for i, model in enumerate(models):
        log.info(f"ğŸ”® Predicting with Fold {i+1} model...")
        fold_predictions = predict_with_tta(model, loader, device, tta_count)
        all_predictions.append(fold_predictions)
    
    # ëª¨ë“  í´ë“œì˜ ì˜ˆì¸¡ì„ í‰ê· 
    ensemble_predictions = np.mean(all_predictions, axis=0)
    final_predictions = np.argmax(ensemble_predictions, axis=1)
    
    return final_predictions

"""## Hyper-parameters
* í•™ìŠµ ë° ì¶”ë¡ ì— í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.
"""

# device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
log.info(f"ğŸ’» Using device: {device}")

# data config
data_path = '../input/data'

# model config
model_name = 'efficientnet_b3'  # ë” ì¢‹ì€ ëª¨ë¸ ì‚¬ìš©

# training config
img_size = 224  # ì´ë¯¸ì§€ í¬ê¸° ëŒ€í­ í™•ëŒ€
LR = 2e-4  # ë” ë‚®ì€ í•™ìŠµë¥ 
EPOCHS = 20  # ì¶©ë¶„í•œ epoch
BATCH_SIZE = 16  # í° ëª¨ë¸ì— ë§ì¶° ë°°ì¹˜ í¬ê¸° ì¡°ì •
num_workers = 0
weight_decay = 1e-4
label_smoothing = 0.1

# K-Fold config
K_FOLDS = 5

"""## Data Preparation
* ë°ì´í„° ë¡œë“œ ë° transform ì •ì˜
"""

# ê°•í™”ëœ augmentationì„ ìœ„í•œ transform ì½”ë“œ
trn_transform = A.Compose([
    A.Resize(height=img_size, width=img_size),
    # ë‹¤ì–‘í•œ ë°ì´í„° ì¦ê°• ê¸°ë²•ë“¤
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.2),
    A.RandomRotate90(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),
    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),
    A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),
    A.Blur(blur_limit=3, p=0.1),
    A.CLAHE(clip_limit=2.0, p=0.2),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2(),
])

# validation image ë³€í™˜ì„ ìœ„í•œ transform ì½”ë“œ
val_transform = A.Compose([
    A.Resize(height=img_size, width=img_size),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2(),
])

# test image ë³€í™˜ì„ ìœ„í•œ transform ì½”ë“œ (TTAìš©ìœ¼ë¡œ ì•½ê°„ì˜ augmentation í¬í•¨)
tst_transform = A.Compose([
    A.Resize(height=img_size, width=img_size),
    A.HorizontalFlip(p=0.5),  # TTAë¥¼ ìœ„í•œ ê°€ë²¼ìš´ augmentation
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2(),
])

# ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv(f"{data_path}/train.csv")
log.info(f"ğŸ“‚ Total training samples: {len(train_df)}")

# í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
class_counts = train_df['target'].value_counts().sort_index()
log.info(f"ğŸ“Š Class distribution: {class_counts.to_dict()}")

"""## K-Fold Cross Validation Training
* 5í´ë“œ ì¸µí™” êµì°¨ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
"""

# K-Fold ì •ì˜
skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)

# í´ë“œë³„ ê²°ê³¼ ì €ì¥
fold_scores = []
best_models = []

log.info(f"\nğŸ”„ Starting {K_FOLDS}-Fold Cross Validation Training...")

for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['target'])):
    log.info(f"\n{'='*60}")
    log.info(f"ğŸ”¥ FOLD {fold+1}/{K_FOLDS}")
    log.info(f"{'='*60}")
    
    # í´ë“œë³„ ë°ì´í„° ë¶„í• 
    train_fold = train_df.iloc[train_idx].reset_index(drop=True)
    val_fold = train_df.iloc[val_idx].reset_index(drop=True)
    
    log.info(f"ğŸ“š Train samples: {len(train_fold)}")
    log.info(f"ğŸ“ Validation samples: {len(val_fold)}")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    trn_dataset = ImageDataset(train_fold, f"{data_path}/train/", transform=trn_transform)
    val_dataset = ImageDataset(val_fold, f"{data_path}/train/", transform=val_transform)
    
    # DataLoader ìƒì„±
    trn_loader = DataLoader(
        trn_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=False
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    # ëª¨ë¸ ì´ˆê¸°í™” (ë§¤ í´ë“œë§ˆë‹¤ ìƒˆë¡œ ì‹œì‘)
    model = timm.create_model(
        model_name,
        pretrained=True,
        num_classes=17
    ).to(device)
    
    log.info(f"ğŸ—ï¸ Model: {model_name}")
    log.info(f"ğŸ”¢ Number of parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # Loss function, optimizer, scheduler ì •ì˜
    loss_fn = LabelSmoothingLoss(classes=17, smoothing=label_smoothing)
    optimizer = Adam(model.parameters(), lr=LR, weight_decay=weight_decay)
    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)
    scaler = GradScaler()
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ì„ ìœ„í•œ ë³€ìˆ˜
    best_val_f1 = 0.0
    best_model_state = None
    
    log.info(f"ğŸš€ Starting Fold {fold+1} training...")
    
    # í´ë“œë³„ í•™ìŠµ
    for epoch in range(EPOCHS):
        log.info(f"\n--- Fold {fold+1} | Epoch {epoch+1}/{EPOCHS} ---")
        
        # Training
        train_ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device, scaler)
        
        # Validation
        val_ret = validate_one_epoch(val_loader, model, loss_fn, device)
        
        # Learning rate scheduler step
        scheduler.step()
        
        # ê²°ê³¼ ì¶œë ¥
        current_lr = optimizer.param_groups[0]['lr']
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_ret['val_f1'] > best_val_f1:
            best_val_f1 = val_ret['val_f1']
            best_model_state = model.state_dict().copy()
            log.info(f"ğŸ’¾ Fold {fold+1} Best model updated! F1: {best_val_f1:.4f}")
        
        # ë¡œê·¸ ì¶œë ¥
        log_msg = f"train_loss: {train_ret['train_loss']:.4f} | "
        log_msg += f"train_acc: {train_ret['train_acc']:.4f} | "
        log_msg += f"train_f1: {train_ret['train_f1']:.4f} | "
        log_msg += f"val_loss: {val_ret['val_loss']:.4f} | "
        log_msg += f"val_acc: {val_ret['val_acc']:.4f} | "
        log_msg += f"val_f1: {val_ret['val_f1']:.4f} | "
        log_msg += f"lr: {current_lr:.6f}"
        
        log.info(log_msg)
    
    # í´ë“œ ì™„ë£Œ
    fold_scores.append(best_val_f1)
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
    model.load_state_dict(best_model_state)
    best_models.append(model.state_dict().copy())
    
    log.info(f"\nğŸ¯ Fold {fold+1} completed! Best F1: {best_val_f1:.4f}")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del model, optimizer, scheduler, scaler, trn_loader, val_loader
    del trn_dataset, val_dataset, train_fold, val_fold
    torch.cuda.empty_cache()
    gc.collect()

# K-Fold ê²°ê³¼ ìš”ì•½
log.info(f"\n{'='*60}")
log.info(f"ğŸ“Š K-FOLD CROSS VALIDATION RESULTS")
log.info(f"{'='*60}")

mean_score = np.mean(fold_scores)
std_score = np.std(fold_scores)

log.info(f"ğŸ“ˆ Individual fold scores: {[f'{score:.4f}' for score in fold_scores]}")
log.info(f"ğŸ¯ Mean F1 Score: {mean_score:.4f}")
log.info(f"ğŸ“ Standard Deviation: {std_score:.4f}")
log.info(f"ğŸ“Š Score Range: {mean_score:.4f} Â± {std_score:.4f}")
log.info(f"â¬‡ï¸ Min Score: {min(fold_scores):.4f}")
log.info(f"â¬†ï¸ Max Score: {max(fold_scores):.4f}")

"""## Ensemble Inference & Save File
* ëª¨ë“  í´ë“œì˜ ëª¨ë¸ì„ ì•™ìƒë¸”í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ì¶”ë¡ ì„ ì§„í–‰í•©ë‹ˆë‹¤.
"""

log.info(f"\nğŸš€ Starting Ensemble Prediction...")

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±
tst_dataset = ImageDataset(
    f"{data_path}/sample_submission.csv",
    f"{data_path}/test/",
    transform=val_transform
)

# TTAë¥¼ ìœ„í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
tst_dataset_tta = ImageDataset(
    f"{data_path}/sample_submission.csv",
    f"{data_path}/test/",
    transform=tst_transform
)

tst_loader_tta = DataLoader(
    tst_dataset_tta,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=0,
    pin_memory=True
)

# ì•™ìƒë¸”ì„ ìœ„í•´ ëª¨ë“  í´ë“œì˜ ëª¨ë¸ ë¡œë“œ
ensemble_models = []
for fold in range(K_FOLDS):
    model = timm.create_model(
        model_name,
        pretrained=True,
        num_classes=17
    ).to(device)
    model.load_state_dict(best_models[fold])
    ensemble_models.append(model)
    log.info(f"âœ… Loaded Fold {fold+1} model")

# ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤í–‰
log.info(f"\nğŸ”® Running ensemble prediction with {K_FOLDS} models and TTA...")
preds_list = predict_ensemble(ensemble_models, tst_loader_tta, device, tta_count=5)

# ê²°ê³¼ ì €ì¥
pred_df = pd.DataFrame(tst_dataset.df, columns=['ID', 'target'])
pred_df['target'] = preds_list

sample_submission_df = pd.read_csv(f"{data_path}/sample_submission.csv")
assert (sample_submission_df['ID'] == pred_df['ID']).all()

output_path = "../output"
os.makedirs(output_path, exist_ok=True)
pred_df.to_csv(f"{output_path}/pred_kfold_ensemble.csv", index=False)

log.info(f"\nâœ… Ensemble prediction completed and saved to {output_path}/pred_kfold_ensemble.csv")
log.info(f"ğŸ“ˆ Final K-Fold CV Score: {mean_score:.4f} Â± {std_score:.4f}")

# ë©”ëª¨ë¦¬ ì •ë¦¬
for model in ensemble_models:
    del model
torch.cuda.empty_cache()
gc.collect()

pred_df.head() 