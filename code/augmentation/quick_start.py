#!/usr/bin/env python3
"""
ë¹ ë¥¸ ì‹œì‘ ì˜ˆì‹œ ìŠ¤í¬ë¦½íŠ¸
í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ê³¼ ê¸°ë³¸ ì‚¬ìš©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def check_data_directories():
    """ë°ì´í„° ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    train_dir = Path("../input/data/train")
    test_dir = Path("../input/data/test")
    
    print("ğŸ“‚ ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸:")
    print(f"  í•™ìŠµ ë°ì´í„°: {train_dir} {'âœ…' if train_dir.exists() else 'âŒ'}")
    print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_dir} {'âœ…' if test_dir.exists() else 'âŒ'}")
    
    if not train_dir.exists():
        print(f"\nâš ï¸ í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {train_dir}")
        print("  ì˜ˆìƒ êµ¬ì¡°:")
        print("  input/data/train/")
        print("  â”œâ”€â”€ class1/")
        print("  â”‚   â”œâ”€â”€ image1.jpg")
        print("  â”‚   â””â”€â”€ ...")
        print("  â””â”€â”€ class2/")
    
    if not test_dir.exists():
        print(f"\nâš ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {test_dir}")
        print("  ì˜ˆìƒ êµ¬ì¡°:")
        print("  input/data/test/")
        print("  â”œâ”€â”€ test1.jpg")
        print("  â”œâ”€â”€ test2.jpg")
        print("  â””â”€â”€ ...")
    
    return train_dir.exists() and test_dir.exists()

def demo_test_analysis():
    """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ ë°ëª¨"""
    print("\nğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ ë°ëª¨")
    print("=" * 40)
    
    try:
        from test_data_analyzer import analyze_document_test_data, save_analysis_results
        
        test_dir = "../input/data/test"
        if not os.path.exists(test_dir):
            print(f"âŒ í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {test_dir}")
            return False
        
        # ë¶„ì„ ì‹¤í–‰ (ìƒ˜í”Œ ìˆ˜ë¥¼ ì¤„ì—¬ì„œ ë¹ ë¥´ê²Œ)
        print("ë¶„ì„ ì¤‘... (ìƒ˜í”Œ ìˆ˜: 20)")
        stats = analyze_document_test_data(test_dir, sample_size=20)
        
        # ê²°ê³¼ ì €ì¥
        save_analysis_results(stats, "demo_analysis.json")
        print("âœ… ë¶„ì„ ì™„ë£Œ! demo_analysis.jsonì— ê²°ê³¼ ì €ì¥")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return False

def demo_augmentation_pipeline():
    """ì¦ê°• íŒŒì´í”„ë¼ì¸ ë°ëª¨"""
    print("\nâš™ï¸ ì¦ê°• íŒŒì´í”„ë¼ì¸ ë°ëª¨")
    print("=" * 40)
    
    try:
        from adaptive_augmentation import create_adaptive_document_pipeline, validate_pipeline_parameters
        import json
        
        # ë¶„ì„ ê²°ê³¼ ë¡œë“œ
        if not os.path.exists("demo_analysis.json"):
            print("âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            return False
        
        with open("demo_analysis.json", 'r') as f:
            stats = json.load(f)
        
        # íŒŒë¼ë¯¸í„° ìœ íš¨ì„± ê²€ì¦
        if not validate_pipeline_parameters(stats):
            print("âŒ íŒŒë¼ë¯¸í„° ìœ íš¨ì„± ê²€ì¦ ì‹¤íŒ¨")
            return False
        
        # íŒŒì´í”„ë¼ì¸ ìƒì„±
        print("íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘...")
        try:
            pipeline = create_adaptive_document_pipeline(stats)
            print("âœ… Augraphy íŒŒì´í”„ë¼ì¸ ìƒì„± ì„±ê³µ!")
            return True
        except Exception as e:
            print(f"âš ï¸ Augraphy íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ pip install augraphy ë¥¼ ì‹¤í–‰í•˜ì—¬ ì„¤ì¹˜í•´ë³´ì„¸ìš”.")
            return False
        
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return False

def demo_dataset():
    """ë°ì´í„°ì…‹ ë°ëª¨"""
    print("\nğŸ“Š ì ì‘í˜• ë°ì´í„°ì…‹ ë°ëª¨")
    print("=" * 40)
    
    try:
        from adaptive_dataset import load_image_paths_and_labels, AdaptiveDocumentDataset
        from torchvision import transforms
        
        train_dir = "../input/data/train"
        test_dir = "../input/data/test"
        
        if not os.path.exists(train_dir):
            print(f"âŒ í•™ìŠµ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {train_dir}")
            return False
        
        # ë°ì´í„° ê²½ë¡œ ë¡œë“œ
        print("ë°ì´í„° ê²½ë¡œ ë¡œë“œ ì¤‘...")
        train_paths, train_labels = load_image_paths_and_labels(train_dir)
        print(f"âœ… í•™ìŠµ ë°ì´í„°: {len(train_paths)}ê°œ ì´ë¯¸ì§€, {len(set(train_labels))}ê°œ í´ë˜ìŠ¤")
        
        # ë³€í™˜ ì •ì˜
        transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # ë°ì´í„°ì…‹ ìƒì„± (ì²« 10ê°œë§Œ ì‚¬ìš©)
        print("ì ì‘í˜• ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
        dataset = AdaptiveDocumentDataset(
            image_paths=train_paths[:10],  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 10ê°œë§Œ
            labels=train_labels[:10],
            test_data_dir=test_dir,
            pytorch_transform=transform,
            cache_analysis=True,
            analysis_cache_path="demo_analysis.json"  # ì´ë¯¸ ìƒì„±ëœ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©
        )
        
        print(f"âœ… ë°ì´í„°ì…‹ ìƒì„± ì„±ê³µ! í¬ê¸°: {len(dataset)}")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ ìš”ì•½:")
        print(dataset.get_test_stats_summary())
        
        return True
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ë°˜ ì ì‘í˜• ì¦ê°• ì‹œìŠ¤í…œ - ë¹ ë¥¸ ì‹œì‘")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸
    if not check_data_directories():
        print("\nâŒ ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •ì„ ì™„ë£Œí•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        print("\nğŸ’¡ ì „ì²´ ì‹œìŠ¤í…œ ì‹¤í–‰:")
        print("   python main.py --train_dir ../input/data/train --test_dir ../input/data/test --analysis_only")
        return
    
    # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„
    if not demo_test_analysis():
        print("\nâŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # 3. ì¦ê°• íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
    if not demo_augmentation_pipeline():
        print("\nâš ï¸ Augraphy íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì§€ë§Œ ê¸°ë³¸ ì¦ê°•ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
    
    # 4. ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸
    if not demo_dataset():
        print("\nâŒ ë°ì´í„°ì…‹ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    print("\nğŸ‰ ëª¨ë“  ë°ëª¨ ì™„ë£Œ!")
    print("\nğŸ“š ë‹¤ìŒ ë‹¨ê³„:")
    print("1. ì „ì²´ ë¶„ì„: python main.py --analysis_only")
    print("2. í•™ìŠµ ì‹¤í–‰: python main.py --epochs 10")
    print("3. ìì„¸í•œ ì‚¬ìš©ë²•: cat README.md")
    
    # ìƒì„±ëœ íŒŒì¼ë“¤ ì •ë¦¬
    cleanup_files = ["demo_analysis.json"]
    print(f"\nğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬: {cleanup_files}")
    for file in cleanup_files:
        if os.path.exists(file):
            os.remove(file)
            print(f"  ì‚­ì œ: {file}")

if __name__ == "__main__":
    main() 