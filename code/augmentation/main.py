"""
í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ë°˜ ì ì‘í˜• ì¦ê°• ì‹œìŠ¤í…œ ë©”ì¸ ì‹¤í–‰ íŒŒì¼
ì‚¬ìš©ë²•: python main.py --train_dir input/data/train --test_dir input/data/test --epochs 50
"""

import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms, models
import os
import time
from pathlib import Path
from typing import Tuple, Dict, Any

# ë¡œì»¬ ëª¨ë“ˆ import
from test_data_analyzer import analyze_document_test_data, save_analysis_results
from adaptive_augmentation import create_adaptive_document_pipeline, validate_pipeline_parameters
from adaptive_dataset import AdaptiveDocumentDataset, load_image_paths_and_labels, create_adaptive_dataloader


class AdaptiveTrainingPipeline:
    """ì ì‘í˜• ì¦ê°• í•™ìŠµ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(
        self,
        train_dir: str,
        test_dir: str,
        num_classes: int,
        device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
        image_size: int = 224,
        batch_size: int = 32,
        learning_rate: float = 0.001
    ):
        self.train_dir = train_dir
        self.test_dir = test_dir
        self.num_classes = num_classes
        self.device = device
        self.image_size = image_size
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        
        print(f"ğŸš€ ì ì‘í˜• í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”")
        print(f"  ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"  ì´ë¯¸ì§€ í¬ê¸°: {self.image_size}")
        print(f"  ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        print(f"  í•™ìŠµë¥ : {self.learning_rate}")
        
        # PyTorch ë³€í™˜ ì •ì˜
        self.pytorch_transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((self.image_size, self.image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # ë°ì´í„° ë¡œë“œ
        self.setup_data()
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.setup_model()
    
    def setup_data(self):
        """ë°ì´í„°ì…‹ ì„¤ì •"""
        print("\nğŸ“ ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # í•™ìŠµ ë°ì´í„° ë¡œë“œ
        self.train_paths, self.train_labels = load_image_paths_and_labels(self.train_dir)
        print(f"  í•™ìŠµ ë°ì´í„°: {len(self.train_paths)}ê°œ ì´ë¯¸ì§€")
        
        # ì ì‘í˜• ë°ì´í„°ì…‹ ìƒì„±
        self.train_dataset = AdaptiveDocumentDataset(
            image_paths=self.train_paths,
            labels=self.train_labels,
            test_data_dir=self.test_dir,
            pytorch_transform=self.pytorch_transform,
            cache_analysis=True,
            analysis_cache_path="test_analysis_results.json"
        )
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        self.train_loader = DataLoader(
            dataset=self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=4,
            pin_memory=True,
            drop_last=True
        )
        
        print(f"  ë°°ì¹˜ ìˆ˜: {len(self.train_loader)}")
        print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ ê²°ê³¼:")
        print(self.train_dataset.get_test_stats_summary())
    
    def setup_model(self):
        """ëª¨ë¸ ë° ìµœì í™” ì„¤ì •"""
        print(f"\nğŸ§  ëª¨ë¸ ì„¤ì • (í´ë˜ìŠ¤ ìˆ˜: {self.num_classes})")
        
        # ResNet ëª¨ë¸ ì‚¬ìš©
        self.model = models.resnet50(pretrained=True)
        
        # ë¶„ë¥˜ í—¤ë“œ êµì²´
        num_features = self.model.fc.in_features
        self.model.fc = nn.Linear(num_features, self.num_classes)
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        self.model = self.model.to(self.device)
        
        # ì†ì‹¤ í•¨ìˆ˜ ë° ìµœì í™”ê¸°
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.AdamW(self.model.parameters(), lr=self.learning_rate, weight_decay=0.01)
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=50)
        
        print("âœ… ëª¨ë¸ ì„¤ì • ì™„ë£Œ")
    
    def train_epoch(self, epoch: int, total_epochs: int) -> Dict[str, float]:
        """í•œ ì—í¬í¬ í•™ìŠµ"""
        
        # ì ì‘í˜• ì¦ê°• ê°•ë„ ì—…ë°ì´íŠ¸
        self.train_dataset.update_pipeline_strength(epoch, total_epochs)
        
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        start_time = time.time()
        
        for batch_idx, (data, target) in enumerate(self.train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
            
            # ì§„í–‰ë¥  ì¶œë ¥
            if batch_idx % 50 == 0:
                print(f'  ë°°ì¹˜ [{batch_idx}/{len(self.train_loader)}] '
                      f'ì†ì‹¤: {loss.item():.4f} | ì •í™•ë„: {100.*correct/total:.2f}%')
        
        epoch_time = time.time() - start_time
        avg_loss = total_loss / len(self.train_loader)
        accuracy = 100. * correct / total
        
        self.scheduler.step()
        
        return {
            'loss': avg_loss,
            'accuracy': accuracy,
            'time': epoch_time
        }
    
    def train(self, epochs: int, save_dir: str = "models"):
        """ì „ì²´ í•™ìŠµ ì‹¤í–‰"""
        print(f"\nğŸ‹ï¸ í•™ìŠµ ì‹œì‘ ({epochs} ì—í¬í¬)")
        
        # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(save_dir).mkdir(exist_ok=True)
        
        best_accuracy = 0
        training_history = []
        
        for epoch in range(epochs):
            print(f"\nğŸ“ˆ ì—í¬í¬ {epoch+1}/{epochs}")
            
            # í•™ìŠµ ì‹¤í–‰
            epoch_results = self.train_epoch(epoch, epochs)
            training_history.append(epoch_results)
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"  í‰ê·  ì†ì‹¤: {epoch_results['loss']:.4f}")
            print(f"  ì •í™•ë„: {epoch_results['accuracy']:.2f}%")
            print(f"  ì†Œìš” ì‹œê°„: {epoch_results['time']:.2f}ì´ˆ")
            print(f"  í•™ìŠµë¥ : {self.scheduler.get_last_lr()[0]:.6f}")
            
            # ìµœê³  ëª¨ë¸ ì €ì¥
            if epoch_results['accuracy'] > best_accuracy:
                best_accuracy = epoch_results['accuracy']
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'accuracy': best_accuracy,
                    'test_stats': self.train_dataset.test_stats
                }, f"{save_dir}/best_model.pth")
                print(f"  â­ ìµœê³  ëª¨ë¸ ì €ì¥ (ì •í™•ë„: {best_accuracy:.2f}%)")
        
        print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ! ìµœê³  ì •í™•ë„: {best_accuracy:.2f}%")
        return training_history
    
    def save_analysis_report(self, output_path: str = "analysis_report.txt"):
        """ë¶„ì„ ë³´ê³ ì„œ ì €ì¥"""
        report = []
        report.append("=" * 60)
        report.append("í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ë°˜ ì ì‘í˜• ì¦ê°• ë¶„ì„ ë³´ê³ ì„œ")
        report.append("=" * 60)
        report.append("")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ ê²°ê³¼
        report.append("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ ê²°ê³¼:")
        report.append("-" * 30)
        for key, stats in self.train_dataset.test_stats.items():
            report.append(f"{key}:")
            report.append(f"  í‰ê· : {stats['mean']:.2f}")
            report.append(f"  í‘œì¤€í¸ì°¨: {stats['std']:.2f}")
            report.append(f"  ë²”ìœ„: {stats['min']:.2f} ~ {stats['max']:.2f}")
            report.append("")
        
        # ë°ì´í„°ì…‹ ì •ë³´
        report.append("ğŸ“ ë°ì´í„°ì…‹ ì •ë³´:")
        report.append("-" * 30)
        report.append(f"í•™ìŠµ ì´ë¯¸ì§€ ìˆ˜: {len(self.train_paths)}")
        report.append(f"í´ë˜ìŠ¤ ìˆ˜: {self.num_classes}")
        report.append(f"ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        report.append("")
        
        # ëª¨ë¸ ì •ë³´
        report.append("ğŸ§  ëª¨ë¸ ì •ë³´:")
        report.append("-" * 30)
        report.append(f"ëª¨ë¸: ResNet-50")
        report.append(f"ë””ë°”ì´ìŠ¤: {self.device}")
        report.append(f"í•™ìŠµë¥ : {self.learning_rate}")
        report.append("")
        
        # ì¦ê°• ì •ë³´
        report.append("âš™ï¸ ì ì‘í˜• ì¦ê°• ì •ë³´:")
        report.append("-" * 30)
        if self.train_dataset.augraphy_available:
            report.append("Augraphy íŒŒì´í”„ë¼ì¸ ì‚¬ìš©")
        else:
            report.append("ê¸°ë³¸ ì¦ê°• ì‚¬ìš© (Augraphy ë¯¸ì„¤ì¹˜)")
        report.append("")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))
        
        print(f"ğŸ“„ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥: {output_path}")


def main():
    parser = argparse.ArgumentParser(description='í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ë°˜ ì ì‘í˜• ì¦ê°• í•™ìŠµ')
    parser.add_argument('--train_dir', type=str, default='input/data/train',
                        help='í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬')
    parser.add_argument('--test_dir', type=str, default='input/data/test',
                        help='í…ŒìŠ¤íŠ¸ ë°ì´í„° ë””ë ‰í† ë¦¬')
    parser.add_argument('--epochs', type=int, default=50,
                        help='í•™ìŠµ ì—í¬í¬ ìˆ˜')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--learning_rate', type=float, default=0.001,
                        help='í•™ìŠµë¥ ')
    parser.add_argument('--image_size', type=int, default=224,
                        help='ì´ë¯¸ì§€ í¬ê¸°')
    parser.add_argument('--num_classes', type=int, default=None,
                        help='í´ë˜ìŠ¤ ìˆ˜ (ìë™ ê°ì§€í•˜ë ¤ë©´ None)')
    parser.add_argument('--device', type=str, default='auto',
                        help='ë””ë°”ì´ìŠ¤ (auto, cuda, cpu)')
    parser.add_argument('--analysis_only', action='store_true',
                        help='ë¶„ì„ë§Œ ìˆ˜í–‰í•˜ê³  í•™ìŠµí•˜ì§€ ì•ŠìŒ')
    
    args = parser.parse_args()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device
    
    print("ğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ë°˜ ì ì‘í˜• ì¦ê°• ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸
    if not os.path.exists(args.train_dir):
        print(f"âŒ í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.train_dir}")
        return
    
    if not os.path.exists(args.test_dir):
        print(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.test_dir}")
        return
    
    # í´ë˜ìŠ¤ ìˆ˜ ìë™ ê°ì§€
    if args.num_classes is None:
        train_paths, train_labels = load_image_paths_and_labels(args.train_dir)
        args.num_classes = len(set(train_labels))
        print(f"ğŸ“Š ìë™ ê°ì§€ëœ í´ë˜ìŠ¤ ìˆ˜: {args.num_classes}")
    
    # ë¶„ì„ë§Œ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°
    if args.analysis_only:
        print("\nğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
        try:
            stats = analyze_document_test_data(args.test_dir, sample_size=100)
            save_analysis_results(stats, "test_analysis_results.json")
            
            # íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸
            if validate_pipeline_parameters(stats):
                pipeline = create_adaptive_document_pipeline(stats)
                print("âœ… ì ì‘í˜• íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return
    
    # ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    try:
        pipeline = AdaptiveTrainingPipeline(
            train_dir=args.train_dir,
            test_dir=args.test_dir,
            num_classes=args.num_classes,
            device=device,
            image_size=args.image_size,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate
        )
        
        # í•™ìŠµ ì‹¤í–‰
        training_history = pipeline.train(args.epochs)
        
        # ë¶„ì„ ë³´ê³ ì„œ ì €ì¥
        pipeline.save_analysis_report()
        
        print("\nğŸ¯ í•™ìŠµ ì™„ë£Œ ìš”ì•½:")
        final_stats = training_history[-1]
        print(f"  ìµœì¢… ì •í™•ë„: {final_stats['accuracy']:.2f}%")
        print(f"  ìµœì¢… ì†ì‹¤: {final_stats['loss']:.4f}")
        
    except Exception as e:
        print(f"âŒ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        print("ë””ë²„ê¹…ì„ ìœ„í•´ --analysis_only ì˜µì…˜ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")


if __name__ == "__main__":
    main() 