# -*- coding: utf-8 -*-
"""baseline_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pjsn3XqvQPx3tqy-ZNsGwYhI0MVkVH7b

# **📄 Document type classification baseline code**
> 문서 타입 분류 대회에 오신 여러분 환영합니다! 🎉     
> 아래 baseline에서는 ResNet 모델을 로드하여, 모델을 학습 및 예측 파일 생성하는 프로세스에 대해 알아보겠습니다.

## Contents
- Prepare Environments
- Import Library & Define Functions
- Hyper-parameters
- Load Data
- Train Model
- Inference & Save File

## 1. Prepare Environments

* 데이터 로드를 위한 구글 드라이브를 마운트합니다.
* 필요한 라이브러리를 설치합니다.
"""

# 구글 드라이브 마운트, Colab을 이용하지 않는다면 패스해도 됩니다.
# from google.colab import drive
# drive.mount('/gdrive', force_remount=True)
# drive.mount('/content/drive')

# 구글 드라이브에 업로드된 대회 데이터를 압축 해제하고 로컬에 저장합니다.
# !tar -xvf drive/MyDrive/datasets_fin.tar > /dev/null

# 필요한 라이브러리를 설치합니다.
# !pip install timm

"""## 2. Import Library & Define Functions
* 학습 및 추론에 필요한 라이브러리를 로드합니다.
* 학습 및 추론에 필요한 함수와 클래스를 정의합니다.
"""

import os
import time
import random
import logging
from datetime import datetime, timezone, timedelta

import timm
import torch
import albumentations as A
import pandas as pd
import numpy as np
import torch.nn as nn
from albumentations.pytorch import ToTensorV2
from torch.optim import Adam
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import train_test_split

# 로깅 설정
class CustomFormatter(logging.Formatter):
    def format(self, record):
        # UTC+9 서울 타임존 설정
        kst = timezone(timedelta(hours=9))
        timestamp = datetime.fromtimestamp(record.created, tz=kst).strftime("%y-%m-%d %H:%M:%S")
        return f"[{timestamp}] [INFO] {record.getMessage()}"

# 로거 설정
log = logging.getLogger(__name__)
log.setLevel(logging.INFO)

# 콘솔 핸들러
console_handler = logging.StreamHandler()
console_handler.setFormatter(CustomFormatter())

# 파일 핸들러 (스크립트명.log로 저장)
script_name = os.path.splitext(os.path.basename(__file__))[0]
script_dir = os.path.dirname(os.path.abspath(__file__))
log_file_path = os.path.join(script_dir, f"{script_name}.log")
file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
file_handler.setFormatter(CustomFormatter())

# 핸들러 추가
log.addHandler(console_handler)
log.addHandler(file_handler)
log.propagate = False

# 시드를 고정합니다.
SEED = 42
os.environ['PYTHONHASHSEED'] = str(SEED)

# CUDA 10.2+ 환경에서 결정적 연산을 위한 환경 변수 설정
os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

# 완전한 재현성을 위한 설정
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# CUDA 환경에서 결정적 연산 설정 (선택적)
try:
    torch.use_deterministic_algorithms(True)
    log.info("완전한 재현성 설정이 활성화되었습니다.")
except Exception as e:
    log.info(f"torch.use_deterministic_algorithms(True) 설정 실패: {e}")
    log.info("기본 재현성 설정만 사용됩니다.")

# 현재 스크립트 위치를 작업 디렉토리로 설정
os.chdir(os.path.dirname(os.path.abspath(__file__)))

# 데이터셋 클래스를 정의합니다.
class ImageDataset(Dataset):
    def __init__(self, csv_data, path, transform=None):
        # csv_data가 DataFrame인 경우와 파일 경로인 경우를 구분
        if isinstance(csv_data, str):
            self.df = pd.read_csv(csv_data).values
        else:
            self.df = csv_data.values
        self.path = path
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        name, target = self.df[idx]
        img = np.array(Image.open(os.path.join(self.path, name)))
        if self.transform:
            img = self.transform(image=img)['image']
        return img, target

# one epoch 학습을 위한 함수입니다.
def train_one_epoch(loader, model, optimizer, loss_fn, device):
    model.train()
    train_loss = 0
    preds_list = []
    targets_list = []

    pbar = tqdm(loader, desc="Training")
    for image, targets in pbar:
        image = image.to(device)
        targets = targets.to(device)

        model.zero_grad(set_to_none=True)

        preds = model(image)
        loss = loss_fn(preds, targets)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())
        targets_list.extend(targets.detach().cpu().numpy())

        pbar.set_description(f"Training Loss: {loss.item():.4f}")

    train_loss /= len(loader)
    train_acc = accuracy_score(targets_list, preds_list)
    train_f1 = f1_score(targets_list, preds_list, average='macro')

    ret = {
        "train_loss": train_loss,
        "train_acc": train_acc,
        "train_f1": train_f1,
    }

    return ret

# one epoch 검증을 위한 함수입니다.
def validate_one_epoch(loader, model, loss_fn, device):
    model.eval()
    valid_loss = 0
    preds_list = []
    targets_list = []

    with torch.no_grad():
        pbar = tqdm(loader, desc="Validation")
        for image, targets in pbar:
            image = image.to(device)
            targets = targets.to(device)

            preds = model(image)
            loss = loss_fn(preds, targets)

            valid_loss += loss.item()
            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())
            targets_list.extend(targets.detach().cpu().numpy())

            pbar.set_description(f"Validation Loss: {loss.item():.4f}")

    valid_loss /= len(loader)
    valid_acc = accuracy_score(targets_list, preds_list)
    valid_f1 = f1_score(targets_list, preds_list, average='macro')

    ret = {
        "valid_loss": valid_loss,
        "valid_acc": valid_acc,
        "valid_f1": valid_f1,
    }

    return ret

"""## 3. Hyper-parameters
* 학습 및 추론에 필요한 하이퍼파라미터들을 정의합니다.
"""

# device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# data config
data_path = '../input/data'

# model config
model_name = 'resnet34' # 'resnet50' 'efficientnet-b0', ...

# training config
img_size = 343
LR = 1e-3
EPOCHS = 50  # early stopping을 위해 충분히 큰 값으로 설정
BATCH_SIZE = 32
num_workers = 0

# early stopping config
PATIENCE = 7  # 성능이 개선되지 않는 epoch 수
MIN_DELTA = 1e-4  # 개선으로 간주할 최소 변화량

"""## 4. Load Data
* 학습, 검증, 테스트 데이터셋과 로더를 정의합니다.
"""

# augmentation을 위한 transform 코드
trn_transform = A.Compose([
    # 이미지 크기 조정
    A.Resize(height=img_size, width=img_size),
    # images normalization
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    # numpy 이미지나 PIL 이미지를 PyTorch 텐서로 변환
    ToTensorV2(),
])

# test image 변환을 위한 transform 코드
tst_transform = A.Compose([
    A.Resize(height=img_size, width=img_size),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2(),
])

# 원본 훈련 데이터 로드
train_df = pd.read_csv(f"{data_path}/train.csv")

# 훈련 데이터를 8:2 비율로 분할
train_data, valid_data = train_test_split(
    train_df, 
    test_size=0.2, 
    random_state=SEED, 
    stratify=train_df['target']  # 클래스 비율을 유지하며 분할
)

log.info(f"훈련 데이터 크기: {len(train_data)}")
log.info(f"검증 데이터 크기: {len(valid_data)}")

# Dataset 정의
trn_dataset = ImageDataset(
    train_data,
    f"{data_path}/train/",
    transform=trn_transform
)
val_dataset = ImageDataset(
    valid_data,
    f"{data_path}/train/",
    transform=tst_transform
)
tst_dataset = ImageDataset(
    f"{data_path}/sample_submission.csv",
    f"{data_path}/test/",
    transform=tst_transform
)

log.info(f"전체 데이터셋 크기: 훈련={len(trn_dataset)}, 검증={len(val_dataset)}, 테스트={len(tst_dataset)}")

# DataLoader 정의
trn_loader = DataLoader(
    trn_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=num_workers,
    pin_memory=True,
    drop_last=False
)
val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=num_workers,
    pin_memory=True,
    drop_last=False
)
tst_loader = DataLoader(
    tst_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=0,
    pin_memory=True
)

"""## 5. Train Model
* 모델을 로드하고, 검증셋을 이용한 early stopping과 함께 학습을 진행합니다.
"""

# load model
model = timm.create_model(
    model_name,
    pretrained=True,
    num_classes=17
).to(device)
loss_fn = nn.CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=LR)

# early stopping을 위한 변수들
best_f1 = 0.0
patience_counter = 0
best_model_state = None

log.info("=" * 60)
log.info("훈련 시작...")
log.info("=" * 60)

for epoch in range(EPOCHS):
    log.info(f"\nEpoch {epoch+1}/{EPOCHS}")
    log.info("-" * 40)
    
    # 훈련
    train_ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device=device)
    
    # 검증
    valid_ret = validate_one_epoch(val_loader, model, loss_fn, device=device)
    
    # 결과 출력
    log.info(f"훈련 - Loss: {train_ret['train_loss']:.4f}, Acc: {train_ret['train_acc']:.4f}, F1: {train_ret['train_f1']:.4f}")
    log.info(f"검증 - Loss: {valid_ret['valid_loss']:.4f}, Acc: {valid_ret['valid_acc']:.4f}, F1: {valid_ret['valid_f1']:.4f}")
    
    # Early stopping 체크
    current_f1 = valid_ret['valid_f1']
    
    if current_f1 > best_f1 + MIN_DELTA:
        best_f1 = current_f1
        patience_counter = 0
        best_model_state = model.state_dict().copy()
        log.info(f"✓ 최고 F1 스코어 갱신: {best_f1:.4f}")
    else:
        patience_counter += 1
        log.info(f"성능 개선 없음 ({patience_counter}/{PATIENCE})")
        
        if patience_counter >= PATIENCE:
            log.info(f"\nEarly stopping! 최고 F1 스코어: {best_f1:.4f}")
            break

# 최고 성능 모델로 복원
if best_model_state is not None:
    model.load_state_dict(best_model_state)
    log.info(f"최고 성능 모델로 복원됨 (F1: {best_f1:.4f})")

"""# 6. Inference & Save File
* 테스트 이미지에 대한 추론을 진행하고, 결과 파일을 저장합니다.
"""

log.info("\n" + "=" * 60)
log.info("테스트 데이터 추론 시작...")
log.info("=" * 60)

preds_list = []

model.eval()
for image, _ in tqdm(tst_loader, desc="테스트 추론"):
    image = image.to(device)

    with torch.no_grad():
        preds = model(image)
    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())

pred_df = pd.DataFrame(tst_dataset.df, columns=['ID', 'target'])
pred_df['target'] = preds_list

sample_submission_df = pd.read_csv(f"{data_path}/sample_submission.csv")
assert (sample_submission_df['ID'] == pred_df['ID']).all()

import os
output_path = "output"
os.makedirs(output_path, exist_ok=True)
pred_df.to_csv(f"{output_path}/pred_valid_holdout.csv", index=False)

log.info(f"\n예측 결과가 {output_path}/pred_valid_holdout.csv에 저장되었습니다.")
log.info(f"최종 모델 검증 F1 스코어: {best_f1:.4f}")

pred_df.head()
