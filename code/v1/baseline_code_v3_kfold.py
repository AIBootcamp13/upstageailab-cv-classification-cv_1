# -*- coding: utf-8 -*-
"""baseline_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pjsn3XqvQPx3tqy-ZNsGwYhI0MVkVH7b

# **ğŸ“„ Document type classification baseline code**
> ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜ ëŒ€íšŒì— ì˜¤ì‹  ì—¬ëŸ¬ë¶„ í™˜ì˜í•©ë‹ˆë‹¤! ğŸ‰     
> ì•„ë˜ baselineì—ì„œëŠ” ResNet ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬, ëª¨ë¸ì„ í•™ìŠµ ë° ì˜ˆì¸¡ íŒŒì¼ ìƒì„±í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

## Contents
- Prepare Environments
- Import Library & Define Functions
- Hyper-parameters
- Load Data
- Train Model
- Inference & Save File

## 1. Prepare Environments

* ë°ì´í„° ë¡œë“œë¥¼ ìœ„í•œ êµ¬ê¸€ ë“œë¼ì´ë¸Œë¥¼ ë§ˆìš´íŠ¸í•©ë‹ˆë‹¤.
* í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
"""

# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸, Colabì„ ì´ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ íŒ¨ìŠ¤í•´ë„ ë©ë‹ˆë‹¤.
# from google.colab import drive
# drive.mount('/gdrive', force_remount=True)
# drive.mount('/content/drive')

# êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì—…ë¡œë“œëœ ëŒ€íšŒ ë°ì´í„°ë¥¼ ì••ì¶• í•´ì œí•˜ê³  ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.
# !tar -xvf drive/MyDrive/datasets_fin.tar > /dev/null

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
# !pip install timm

"""## 2. Import Library & Define Functions
* í•™ìŠµ ë° ì¶”ë¡ ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
* í•™ìŠµ ë° ì¶”ë¡ ì— í•„ìš”í•œ í•¨ìˆ˜ì™€ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
"""

import os
import time
import random

import timm
import torch
import albumentations as A
import pandas as pd
import numpy as np
import torch.nn as nn
from albumentations.pytorch import ToTensorV2
from torch.optim import Adam
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import train_test_split, StratifiedKFold

# ë¡œê·¸ ìœ í‹¸ë¦¬í‹° import
import utils.log_util as log


# ì‹œë“œë¥¼ ê³ ì •í•©ë‹ˆë‹¤.
SEED = 42
os.environ['PYTHONHASHSEED'] = str(SEED)

# CUDA 10.2+ í™˜ê²½ì—ì„œ ê²°ì •ì  ì—°ì‚°ì„ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

# ì™„ì „í•œ ì¬í˜„ì„±ì„ ìœ„í•œ ì„¤ì •
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# CUDA í™˜ê²½ì—ì„œ ê²°ì •ì  ì—°ì‚° ì„¤ì • (ì„ íƒì )
try:
    torch.use_deterministic_algorithms(True)
    log.info("ì™„ì „í•œ ì¬í˜„ì„± ì„¤ì •ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    log.info(f"torch.use_deterministic_algorithms(True) ì„¤ì • ì‹¤íŒ¨: {e}")
    log.info("ê¸°ë³¸ ì¬í˜„ì„± ì„¤ì •ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ë¥¼ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
os.chdir(os.path.dirname(os.path.abspath(__file__)))

# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
class ImageDataset(Dataset):
    def __init__(self, csv_data, path, transform=None):
        # csv_dataê°€ DataFrameì¸ ê²½ìš°ì™€ íŒŒì¼ ê²½ë¡œì¸ ê²½ìš°ë¥¼ êµ¬ë¶„
        if isinstance(csv_data, str):
            self.df = pd.read_csv(csv_data).values
        else:
            self.df = csv_data.values
        self.path = path
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        name, target = self.df[idx]
        img = np.array(Image.open(os.path.join(self.path, name)))
        if self.transform:
            img = self.transform(image=img)['image']
        return img, target

# one epoch í•™ìŠµì„ ìœ„í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤.
def train_one_epoch(loader, model, optimizer, loss_fn, device):
    model.train()
    train_loss = 0
    preds_list = []
    targets_list = []

    pbar = tqdm(loader, desc="Training")
    for image, targets in pbar:
        image = image.to(device)
        targets = targets.to(device)

        model.zero_grad(set_to_none=True)

        preds = model(image)
        loss = loss_fn(preds, targets)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())
        targets_list.extend(targets.detach().cpu().numpy())

        pbar.set_description(f"Training Loss: {loss.item():.4f}")

    train_loss /= len(loader)
    train_acc = accuracy_score(targets_list, preds_list)
    train_f1 = f1_score(targets_list, preds_list, average='macro')

    ret = {
        "train_loss": train_loss,
        "train_acc": train_acc,
        "train_f1": train_f1,
    }

    return ret

# one epoch ê²€ì¦ì„ ìœ„í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤.
def validate_one_epoch(loader, model, loss_fn, device):
    model.eval()
    valid_loss = 0
    preds_list = []
    targets_list = []

    with torch.no_grad():
        pbar = tqdm(loader, desc="Validation")
        for image, targets in pbar:
            image = image.to(device)
            targets = targets.to(device)

            preds = model(image)
            loss = loss_fn(preds, targets)

            valid_loss += loss.item()
            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())
            targets_list.extend(targets.detach().cpu().numpy())

            pbar.set_description(f"Validation Loss: {loss.item():.4f}")

    valid_loss /= len(loader)
    valid_acc = accuracy_score(targets_list, preds_list)
    valid_f1 = f1_score(targets_list, preds_list, average='macro')

    ret = {
        "valid_loss": valid_loss,
        "valid_acc": valid_acc,
        "valid_f1": valid_f1,
    }

    return ret

"""## 3. Hyper-parameters
* í•™ìŠµ ë° ì¶”ë¡ ì— í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.
"""

# device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# data config
data_path = '../input/data'

# model config
model_name = 'resnet34' # 'resnet50' 'efficientnet-b0', ...

# training config
img_size = 343
LR = 1e-3
EPOCHS = 50  # early stoppingì„ ìœ„í•´ ì¶©ë¶„íˆ í° ê°’ìœ¼ë¡œ ì„¤ì •
BATCH_SIZE = 32
num_workers = 0

# early stopping config
PATIENCE = 7  # ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•ŠëŠ” epoch ìˆ˜
MIN_DELTA = 1e-4  # ê°œì„ ìœ¼ë¡œ ê°„ì£¼í•  ìµœì†Œ ë³€í™”ëŸ‰

"""## 4. Load Data
* í•™ìŠµ, ê²€ì¦, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ê³¼ ë¡œë”ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
"""

# augmentationì„ ìœ„í•œ transform ì½”ë“œ
trn_transform = A.Compose([
    # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
    A.Resize(height=img_size, width=img_size),
    # images normalization
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    # numpy ì´ë¯¸ì§€ë‚˜ PIL ì´ë¯¸ì§€ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜
    ToTensorV2(),
])

# test image ë³€í™˜ì„ ìœ„í•œ transform ì½”ë“œ
tst_transform = A.Compose([
    A.Resize(height=img_size, width=img_size),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2(),
])

# ì›ë³¸ í›ˆë ¨ ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv(f"{data_path}/train.csv")

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì •ì˜ (K-Fold ì™¸ë¶€ì—ì„œ í•œ ë²ˆë§Œ ì •ì˜)
tst_dataset = ImageDataset(
    f"{data_path}/sample_submission.csv",
    f"{data_path}/test/",
    transform=tst_transform
)
tst_loader = DataLoader(
    tst_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=0,
    pin_memory=True
)

log.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í¬ê¸°: {len(tst_dataset)}")

# K-Fold ì„¤ì •
N_SPLITS = 5 # K-Foldì˜ ë¶„í•  ìˆ˜
skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)

all_fold_best_f1 = [] # ê° í´ë“œì˜ ìµœê³  F1 ìŠ¤ì½”ì–´ë¥¼ ì €ì¥
model_save_paths = [] # ê° í´ë“œì˜ ëª¨ë¸ ì €ì¥ ê²½ë¡œë¥¼ ì €ì¥

# ëª¨ë¸ ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ìƒì„±
model_save_dir = "saved_models"
os.makedirs(model_save_dir, exist_ok=True)

for fold, (train_index, val_index) in enumerate(skf.split(train_df['ID'], train_df['target'])):
    log.info(f"\n{'=' * 60}")
    log.info(f"Fold {fold+1}/{N_SPLITS} í›ˆë ¨ ì‹œì‘...")
    log.info(f"{'=' * 60}")

    train_data = train_df.iloc[train_index]
    valid_data = train_df.iloc[val_index]

    log.info(f"Fold {fold+1} í›ˆë ¨ ë°ì´í„° í¬ê¸°: {len(train_data)}")
    log.info(f"Fold {fold+1} ê²€ì¦ ë°ì´í„° í¬ê¸°: {len(valid_data)}")

    # Dataset ì •ì˜
    trn_dataset = ImageDataset(
        train_data,
        f"{data_path}/train/",
        transform=trn_transform
    )
    val_dataset = ImageDataset(
        valid_data,
        f"{data_path}/train/",
        transform=tst_transform
    )

    # DataLoader ì •ì˜
    trn_loader = DataLoader(
        trn_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=False
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=False
    )

    # ëª¨ë¸ ë¡œë“œ (ê° í´ë“œë§ˆë‹¤ ìƒˆë¡œ ë¡œë“œí•˜ì—¬ ë…ë¦½ì ì¸ í•™ìŠµ ë³´ì¥)
    model = timm.create_model(
        model_name,
        pretrained=True,
        num_classes=17
    ).to(device)
    loss_fn = nn.CrossEntropyLoss()
    optimizer = Adam(model.parameters(), lr=LR)

    # early stoppingì„ ìœ„í•œ ë³€ìˆ˜ë“¤
    best_f1 = 0.0
    patience_counter = 0
    best_model_state = None

    for epoch in range(EPOCHS):
        log.info(f"\nFold {fold+1} - Epoch {epoch+1}/{EPOCHS}")
        log.info("-" * 40)
        
        # í›ˆë ¨
        train_ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device=device)
        
        # ê²€ì¦
        valid_ret = validate_one_epoch(val_loader, model, loss_fn, device=device)
        
        # ê²°ê³¼ ì¶œë ¥
        log.info(f"í›ˆë ¨ - Loss: {train_ret['train_loss']:.4f}, Acc: {train_ret['train_acc']:.4f}, F1: {train_ret['train_f1']:.4f}")
        log.info(f"ê²€ì¦ - Loss: {valid_ret['valid_loss']:.4f}, Acc: {valid_ret['valid_acc']:.4f}, F1: {valid_ret['valid_f1']:.4f}")
        
        # Early stopping ì²´í¬
        current_f1 = valid_ret['valid_f1']
        
        if current_f1 > best_f1 + MIN_DELTA:
            best_f1 = current_f1
            patience_counter = 0
            best_model_state = model.state_dict().copy()
            log.info(f"âœ“ ìµœê³  F1 ìŠ¤ì½”ì–´ ê°±ì‹ : {best_f1:.4f}")
        else:
            patience_counter += 1
            log.info(f"ì„±ëŠ¥ ê°œì„  ì—†ìŒ ({patience_counter}/{PATIENCE})")
            
            if patience_counter >= PATIENCE:
                log.info(f"\nEarly stopping! ìµœê³  F1 ìŠ¤ì½”ì–´: {best_f1:.4f}")
                break

    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        model_save_path = os.path.join(model_save_dir, f"fold_{fold+1}_best_model.pth")
        torch.save(best_model_state, model_save_path)
        model_save_paths.append(model_save_path)
        log.info(f"Fold {fold+1} - ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ë¨: {model_save_path} (F1: {best_f1:.4f})")
    else:
        # í˜¹ì‹œ best_model_stateê°€ Noneì¸ ê²½ìš° í˜„ì¬ ëª¨ë¸ ìƒíƒœ ì €ì¥
        model_save_path = os.path.join(model_save_dir, f"fold_{fold+1}_best_model.pth")
        torch.save(model.state_dict(), model_save_path)
        model_save_paths.append(model_save_path)
        log.info(f"Fold {fold+1} - í˜„ì¬ ëª¨ë¸ ìƒíƒœ ì €ì¥ë¨: {model_save_path} (F1: {best_f1:.4f})")
    
    all_fold_best_f1.append(best_f1)

log.info(f"\n{'=' * 60}")
log.info(f"ëª¨ë“  í´ë“œ í›ˆë ¨ ì™„ë£Œ. ê° í´ë“œì˜ ìµœê³  F1 ìŠ¤ì½”ì–´: {all_fold_best_f1}")
log.info(f"í‰ê·  F1 ìŠ¤ì½”ì–´: {np.mean(all_fold_best_f1):.4f}")
log.info(f"ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ: {model_save_paths}")
log.info(f"{'=' * 60}")

"""# 6. Inference & Save File
* í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ì¶”ë¡ ì„ ì§„í–‰í•˜ê³ , ê²°ê³¼ íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤.
* K-Fold ì•™ìƒë¸”ì„ í†µí•´ ëª¨ë“  í´ë“œ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ í‰ê· ë‚´ì–´ ìµœì¢… ì˜ˆì¸¡ì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

log.info("\n" + "=" * 60)
log.info("K-Fold ì•™ìƒë¸” í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  ì‹œì‘...")
log.info("=" * 60)

# ëª¨ë“  í´ë“œ ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
all_fold_probs = []

for fold_idx, model_path in enumerate(model_save_paths):
    log.info(f"Fold {fold_idx+1} ëª¨ë¸ë¡œ ì¶”ë¡  ì¤‘...")
    
    # ëª¨ë¸ ë¡œë“œ
    model = timm.create_model(
        model_name,
        pretrained=False,  # ì €ì¥ëœ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ pretrained=False
        num_classes=17
    ).to(device)
    
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    
    fold_probs = []
    
    with torch.no_grad():
        for image, _ in tqdm(tst_loader, desc=f"Fold {fold_idx+1} ì¶”ë¡ "):
            image = image.to(device)
            preds = model(image)
            # ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì ìš©í•˜ì—¬ í™•ë¥ ë¡œ ë³€í™˜
            probs = torch.softmax(preds, dim=1)
            fold_probs.extend(probs.detach().cpu().numpy())
    
    all_fold_probs.append(np.array(fold_probs))
    log.info(f"Fold {fold_idx+1} ì¶”ë¡  ì™„ë£Œ")

# ëª¨ë“  í´ë“œì˜ í™•ë¥ ì„ í‰ê· ë‚´ì–´ ì•™ìƒë¸” ì˜ˆì¸¡ ìƒì„±
log.info("ì•™ìƒë¸” ì˜ˆì¸¡ ìƒì„± ì¤‘...")
ensemble_probs = np.mean(all_fold_probs, axis=0)
ensemble_preds = np.argmax(ensemble_probs, axis=1)

# ê²°ê³¼ DataFrame ìƒì„±
pred_df = pd.DataFrame(tst_dataset.df, columns=['ID', 'target'])
pred_df['target'] = ensemble_preds

# ê²€ì¦
sample_submission_df = pd.read_csv(f"{data_path}/sample_submission.csv")
assert (sample_submission_df['ID'] == pred_df['ID']).all()

# ê²°ê³¼ ì €ì¥
output_path = "output"
os.makedirs(output_path, exist_ok=True)
pred_df.to_csv(f"{output_path}/pred_kfold_ensemble.csv", index=False)

log.info(f"\nì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼ê°€ {output_path}/pred_kfold_ensemble.csvì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
log.info(f"K-Fold ì•™ìƒë¸” ê²€ì¦ í‰ê·  F1 ìŠ¤ì½”ì–´: {np.mean(all_fold_best_f1):.4f}")
log.info(f"ì‚¬ìš©ëœ í´ë“œ ìˆ˜: {len(model_save_paths)}")

# ì¶”ê°€ë¡œ ê° í´ë“œë³„ ì˜ˆì¸¡ í™•ë¥ ë„ ì €ì¥ (ë””ë²„ê¹…/ë¶„ì„ìš©)
ensemble_probs_df = pd.DataFrame(ensemble_probs)
ensemble_probs_df.insert(0, 'ID', pred_df['ID'].values)
ensemble_probs_df.to_csv(f"{output_path}/ensemble_probabilities.csv", index=False)
log.info(f"ì•™ìƒë¸” í™•ë¥  ê²°ê³¼ê°€ {output_path}/ensemble_probabilities.csvì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

pred_df.head()
